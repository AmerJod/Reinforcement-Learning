{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning CartPole with Q-Learning!\n",
    "This notebook will tackle the game of CartPole using a value-based reinforcement learning method known as Q-Learning! The CartPole environment is readily available through OpenAI's gym module.\n",
    "\n",
    "## Q-Learning\n",
    "The idea of value-based reinforcement learning methods are to indirectly find the optimal-policy of a task by first estimating the worth of a particular state under some policy governed by these values. More succinctly, the expected future returns of each state are calculated and a policy is found by (usually) acting greedily with respect to these values.\n",
    "\n",
    "Assuming that our system is a Markov Decision Process (MDP), the expected reward for being in a particular state s and following some fixed policy $\\pi$ can be expressed by the Bellman Equation:\n",
    "\n",
    "$$V^{\\pi}(s) = \\sum_{a} \\pi (s, a) \\sum_{s'} \\mathcal{P}_{s s'}^{a} \\Big[ \\mathcal{R}_{s s'}^{a} + \\gamma V^{\\pi}(s') \\Big] $$\n",
    "\n",
    "where $\\mathcal{R}_{s s'}^{a}$ is the reward function, $\\gamma$ is the discount factor, $\\mathcal{P}_{s s'}^{a}$ are the transition probabilties, and $V^{\\pi}(*)$ is the state-value function under a policy, $\\pi$.\n",
    "\n",
    "By choosing the action that gives rise to the highest expected return (acting greedily), we can then write the Bellman Equation that mimics this behaviour, known as the Bellman Optimality Equation.\n",
    "\n",
    "$$V^{\\pi}(s) = \\max_a\\sum_{s'} \\mathcal{P}_{s s'}^{a} \\Big[ \\mathcal{R}_{s s'}^{a} + \\gamma V^{\\pi}(s') \\Big] $$\n",
    "\n",
    "Generally, if the MDP is fully-defined it is possible to solve for these state-values for all states using dynamic programming. Unfortunately, in most cases, the MDP is not completely known and therefore other methods are needed to solve the Bellman Equations. \n",
    "\n",
    "One such method of solving the Bellman Optimality Equation is Q-learning. It is an off-policy TD-learning method that uses gradient descent updates at every step when the agent is exploring the environment. \n",
    "\n",
    "$$Q_{t+1}^\\pi(s_n, a_n) = Q_t^\\pi(s_n, a_n) + \\alpha \\bigg[\\mathcal{R}_{s s'}^{a} + \\gamma \\max_{a'} Q_t^\\pi(s_{n+1}, a') - Q_t^\\pi(s_n, a_n)\\bigg]$$\n",
    "\n",
    "When dealing with a discrete state-space, action-values can be stored in a table -- but when dealing with continous state-spaces, this becomes intractable. As a result, the action-value function has to be approximated. Neural networks are great function-approximators, and such they are a good tool to use for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from q_learning import PolicyNetwork, QAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "    \n",
    "state_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "batch_size = 256\n",
    "hidden_size = 32\n",
    "memory_flush = 2000\n",
    "\n",
    "discount = 0.99\n",
    "episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 -> Loss: 0.0000\t Reward: 9 \t(eps: 0.0100)\n",
      "Episode 20 -> Loss: 0.0000\t Reward: 10 \t(eps: 0.0100)\n",
      "Episode 40 -> Loss: 1008.2428\t Reward: 31 \t(eps: 0.0100)\n",
      "Episode 60 -> Loss: 613.7448\t Reward: 37 \t(eps: 0.0100)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-763b16d6edee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_current\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "number_of_runs = 10\n",
    "run_rewards = []\n",
    "\n",
    "for run in range(number_of_runs):\n",
    "    policy_network = PolicyNetwork(state_space, action_space, hidden_size).double()\n",
    "    target_network = PolicyNetwork(state_space, action_space, hidden_size).double()\n",
    "\n",
    "    agent = QAgent(policy_network, target_network, epsilon=0.01)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimiser = torch.optim.RMSprop(policy_network.parameters())\n",
    "    \n",
    "    episode_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = torch.tensor(env.reset())\n",
    "        score = 0\n",
    "        loss = 0\n",
    "\n",
    "        if episodes % 10 == 0:\n",
    "            target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            action = agent.select_action(state, action_space)\n",
    "            next_state, reward, done, info = env.step(action.data.numpy()[0, 0])\n",
    "            next_state = torch.tensor(next_state).type(torch.DoubleTensor)\n",
    "\n",
    "            reward = torch.tensor(reward).type(torch.FloatTensor).view(-1, 1)\n",
    "\n",
    "            score += reward\n",
    "\n",
    "            agent.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "            if len(agent.memory) > memory_flush:\n",
    "                del agent.memory[0]\n",
    "\n",
    "            if len(agent.memory) > batch_size:\n",
    "                minibatch = random.sample(agent.memory, batch_size)\n",
    "                batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*minibatch)\n",
    "\n",
    "                batch_state = torch.stack(batch_state)\n",
    "                batch_action = torch.stack(batch_action).view(-1, 1)\n",
    "                batch_reward = torch.stack(batch_reward).view(-1, 1)\n",
    "                batch_next_state = torch.stack(batch_next_state)\n",
    "\n",
    "                Q_next_max, _ = torch.max(target_network.forward(batch_next_state).detach(), dim=1)\n",
    "                Q_next_max = Q_next_max.view(-1, 1).detach()\n",
    "\n",
    "                discounted_Q = (discount * Q_next_max).type(torch.FloatTensor)\n",
    "                batch_reward = batch_reward.type(torch.FloatTensor)\n",
    "\n",
    "                Q_current = policy_network.forward(batch_state)\n",
    "                Q_target = Q_current.clone()\n",
    "\n",
    "                for i in range(len(Q_target)):\n",
    "                    if not batch_done[i]:\n",
    "                        Q_target[i, batch_action[i][0]] = batch_reward[i] + discounted_Q[i]\n",
    "\n",
    "                    else:\n",
    "                        Q_target[i, batch_action[i][0]] = batch_reward[i]\n",
    "\n",
    "                loss = criterion(Q_current, Q_target)\n",
    "\n",
    "                loss.backward()\n",
    "                optimiser.step()\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done and run == 0 and episode % 20 == 0:\n",
    "                print(\"Episode %d -> Loss: %.4f\\t Reward: %d \\t(eps: %.4f)\" % (episode, loss, score, agent.epsilon))\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(score.data.numpy()[0, 0])\n",
    "    \n",
    "    run_rewards.append(episode_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "def moving_average(a, n=20) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "means = np.mean([moving_average(rewards) for rewards in run_rewards], axis=0)\n",
    "stds = np.std([moving_average(rewards) for rewards in run_rewards], axis=0)\n",
    "\n",
    "plt.plot(range(len(means)), means, color='red')\n",
    "\n",
    "plt.fill_between(range(len(means)), means, means + stds, color='blue', alpha=.25)\n",
    "plt.fill_between(range(len(means)), means, means - stds, color='blue', alpha=.25)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
